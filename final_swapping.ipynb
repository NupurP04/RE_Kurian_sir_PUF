{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YEcR_4-5Wzaz",
        "outputId": "14f27fd0-93ca-4152-8dec-ce06efaf2f25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.12\n",
            "  Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting keras==2.12\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting larq\n",
            "  Downloading larq-0.13.3-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (25.2.10)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.12)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.71.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.13.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.5.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (18.1.1)\n",
            "Collecting numpy<1.24,>=1.22 (from tensorflow==2.12)\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (24.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.12)\n",
            "  Downloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.17.0)\n",
            "Collecting tensorboard<2.13,>=2.12 (from tensorflow==2.12)\n",
            "  Downloading tensorboard-2.12.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow==2.12)\n",
            "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (4.13.1)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.12)\n",
            "  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.37.1)\n",
            "Collecting terminaltables>=3.1.0 (from larq)\n",
            "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.5.2,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (0.5.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (0.4.1)\n",
            "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.5.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.3,>=0.5.3 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.5.3-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.5.1-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.5.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.0,>=0.5.0 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.5.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (978 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.38-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.38,>=0.4.38 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.38-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.37-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.37,>=0.4.36 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.36-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.36-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.35-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "INFO: pip is still looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.34,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.34-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.33-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.33,>=0.4.33 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.33-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.31,>=0.4.30 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.31-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.30,>=0.4.27 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (1.14.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.38.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow==2.12)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.2.2)\n",
            "Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (586.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.0/586.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading larq-0.13.3-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.2/66.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl (79.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, terminaltables, tensorflow-estimator, protobuf, numpy, keras, gast, larq, jaxlib, google-auth-oauthlib, tensorboard, jax, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.8.0\n",
            "    Uninstalling keras-3.8.0:\n",
            "      Successfully uninstalled keras-3.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.6.0\n",
            "    Uninstalling gast-0.6.0:\n",
            "      Successfully uninstalled gast-0.6.0\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.5.1\n",
            "    Uninstalling jaxlib-0.5.1:\n",
            "      Successfully uninstalled jaxlib-0.5.1\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.5.2\n",
            "    Uninstalling jax-0.5.2:\n",
            "      Successfully uninstalled jax-0.5.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "blosc2 3.2.1 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.6 which is incompatible.\n",
            "bigframes 1.42.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.12.0 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.6 which is incompatible.\n",
            "flax 0.10.5 requires jax>=0.5.1, but you have jax 0.4.30 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "xarray 2025.1.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "orbax-checkpoint 0.11.10 requires jax>=0.5.0, but you have jax 0.4.30 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-1.0.0 jax-0.4.30 jaxlib-0.4.30 keras-2.12.0 larq-0.13.3 numpy-1.23.5 protobuf-4.25.6 tensorboard-2.12.3 tensorflow-2.12.0 tensorflow-estimator-2.12.0 terminaltables-3.1.10 wrapt-1.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "f774ac4d0af1438dad9fd4351068146f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install tensorflow==2.12 keras==2.12 larq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import larq as lq\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Loading and preprocessing MNIST\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "train_images = train_images.reshape(-1, 28 * 28).astype(\"float32\") / 127.5 - 1\n",
        "test_images = test_images.reshape(-1, 28 * 28).astype(\"float32\") / 127.5 - 1\n",
        "\n",
        "train_labels = tf.keras.utils.to_categorical(train_labels, 10)\n",
        "test_labels = tf.keras.utils.to_categorical(test_labels, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b36VEu5yXL0O",
        "outputId": "af847d48-a15d-49cf-b6fd-b48867865be2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Binarized layer parameters\n",
        "kwargs = dict(\n",
        "    input_quantizer=\"ste_sign\",\n",
        "    kernel_quantizer=\"ste_sign\",\n",
        "    kernel_constraint=\"weight_clip\",\n",
        "    use_bias=False\n",
        ")\n",
        "\n",
        "# Defining model from the paper\n",
        "model = tf.keras.models.Sequential([\n",
        "    lq.layers.QuantDense(512, input_shape=(784,), **kwargs),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "\n",
        "    lq.layers.QuantDense(512, **kwargs),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "\n",
        "    lq.layers.QuantDense(512, **kwargs),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "\n",
        "    lq.layers.QuantDense(10, **kwargs),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Activation(\"softmax\")\n",
        "])"
      ],
      "metadata": {
        "id": "qHqwwa4EXWuT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile and train\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(train_images, train_labels, batch_size=128, epochs=10, validation_data=(test_images, test_labels))\n",
        "\n",
        "# Evaluating the original model\n",
        "print(\"\\n Original Model Accuracy \")\n",
        "original_loss, original_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"Test Accuracy: {original_acc*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzZtg60MXaym",
        "outputId": "d68daa1f-a558-42ab-ab99-ab4e395191c9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "469/469 [==============================] - 22s 43ms/step - loss: 0.1506 - accuracy: 0.9561 - val_loss: 0.1468 - val_accuracy: 0.9554\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 20s 42ms/step - loss: 0.0792 - accuracy: 0.9772 - val_loss: 0.1384 - val_accuracy: 0.9585\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 19s 41ms/step - loss: 0.0671 - accuracy: 0.9806 - val_loss: 0.1553 - val_accuracy: 0.9535\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 18s 39ms/step - loss: 0.0592 - accuracy: 0.9823 - val_loss: 0.1422 - val_accuracy: 0.9590\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 19s 40ms/step - loss: 0.0514 - accuracy: 0.9843 - val_loss: 0.1425 - val_accuracy: 0.9594\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 18s 39ms/step - loss: 0.0528 - accuracy: 0.9836 - val_loss: 0.1202 - val_accuracy: 0.9666\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 22s 47ms/step - loss: 0.0464 - accuracy: 0.9862 - val_loss: 0.1216 - val_accuracy: 0.9645\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 20s 43ms/step - loss: 0.0430 - accuracy: 0.9871 - val_loss: 0.1136 - val_accuracy: 0.9677\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 19s 41ms/step - loss: 0.0415 - accuracy: 0.9877 - val_loss: 0.1204 - val_accuracy: 0.9687\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 18s 39ms/step - loss: 0.0372 - accuracy: 0.9887 - val_loss: 0.1289 - val_accuracy: 0.9639\n",
            "\n",
            " Original Model Accuracy \n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.1289 - accuracy: 0.9639\n",
            "Test Accuracy: 96.39%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract and print binary weights\n",
        "def get_binarized_weights(layer):\n",
        "    weights = layer.get_weights()[0]\n",
        "    quantizer = layer.kernel_quantizer  # Directly using kernel_quantizer of the QuantDense layer\n",
        "    binarized_weights = quantizer(weights)\n",
        "    return binarized_weights.numpy()"
      ],
      "metadata": {
        "id": "rYrHrPNFXlF8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing binary weights before column swap\n",
        "print(\"\\n Binarised Weights Before Column Swap:\")\n",
        "for layer in model.layers:\n",
        "    if isinstance(layer, lq.layers.QuantDense):\n",
        "        print(f\"\\nLayer: {layer.name}\")\n",
        "        binary_weights = get_binarized_weights(layer)\n",
        "        print(binary_weights[:10, :10])  # Print top-left 10x10 part of the binary weights\n",
        "\n",
        "# Save model with binary weights\n",
        "os.makedirs(\"model\", exist_ok=True)\n",
        "model.save(\"model/binarized_model\", include_optimizer=False)\n",
        "print(\"\\n Model saved with binarised weights.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKizGtB9XsMI",
        "outputId": "b0e1246b-b9b8-472a-ae4f-82ed2a93c531"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Binarised Weights Before Column Swap:\n",
            "\n",
            "Layer: quant_dense\n",
            "[[-1. -1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
            " [ 1. -1.  1. -1. -1.  1.  1. -1. -1.  1.]\n",
            " [ 1.  1. -1.  1. -1. -1. -1.  1. -1. -1.]\n",
            " [ 1.  1.  1.  1. -1. -1. -1.  1.  1.  1.]\n",
            " [ 1.  1. -1. -1. -1.  1. -1. -1. -1. -1.]\n",
            " [ 1. -1.  1. -1.  1.  1.  1.  1.  1. -1.]\n",
            " [ 1.  1.  1. -1.  1. -1. -1. -1.  1. -1.]\n",
            " [-1. -1.  1.  1.  1. -1. -1. -1. -1.  1.]\n",
            " [-1. -1.  1. -1. -1.  1. -1.  1.  1. -1.]\n",
            " [-1. -1. -1. -1. -1.  1.  1. -1. -1.  1.]]\n",
            "\n",
            "Layer: quant_dense_1\n",
            "[[ 1. -1. -1.  1.  1.  1. -1. -1.  1.  1.]\n",
            " [-1. -1. -1. -1. -1.  1.  1. -1. -1.  1.]\n",
            " [-1.  1.  1. -1. -1.  1. -1.  1. -1.  1.]\n",
            " [-1. -1. -1.  1. -1. -1.  1.  1.  1. -1.]\n",
            " [ 1.  1.  1. -1.  1. -1.  1.  1.  1.  1.]\n",
            " [ 1.  1. -1.  1. -1. -1.  1.  1.  1. -1.]\n",
            " [-1.  1.  1. -1.  1.  1. -1. -1. -1.  1.]\n",
            " [ 1.  1.  1. -1. -1. -1.  1. -1.  1. -1.]\n",
            " [-1. -1. -1. -1. -1. -1.  1. -1. -1. -1.]\n",
            " [ 1. -1.  1. -1.  1.  1. -1. -1. -1.  1.]]\n",
            "\n",
            "Layer: quant_dense_2\n",
            "[[-1. -1. -1. -1.  1.  1.  1. -1. -1.  1.]\n",
            " [ 1. -1. -1.  1.  1.  1. -1. -1.  1.  1.]\n",
            " [-1. -1. -1.  1. -1. -1.  1.  1.  1. -1.]\n",
            " [ 1. -1.  1.  1.  1. -1.  1.  1. -1. -1.]\n",
            " [-1.  1. -1. -1.  1.  1.  1. -1.  1.  1.]\n",
            " [-1.  1. -1.  1. -1. -1. -1. -1.  1. -1.]\n",
            " [ 1.  1.  1. -1. -1. -1. -1.  1. -1. -1.]\n",
            " [ 1. -1.  1. -1. -1.  1. -1. -1. -1.  1.]\n",
            " [ 1. -1.  1.  1. -1. -1.  1. -1. -1.  1.]\n",
            " [-1. -1.  1. -1. -1. -1. -1.  1.  1. -1.]]\n",
            "\n",
            "Layer: quant_dense_3\n",
            "[[ 1.  1.  1.  1.  1.  1. -1.  1. -1. -1.]\n",
            " [-1.  1.  1. -1.  1.  1.  1.  1. -1. -1.]\n",
            " [ 1.  1.  1.  1. -1.  1. -1. -1.  1. -1.]\n",
            " [-1. -1.  1.  1. -1.  1. -1.  1.  1.  1.]\n",
            " [-1.  1. -1.  1.  1. -1.  1.  1. -1. -1.]\n",
            " [ 1. -1. -1.  1.  1. -1.  1.  1. -1. -1.]\n",
            " [ 1.  1. -1.  1. -1. -1.  1.  1.  1. -1.]\n",
            " [ 1.  1.  1. -1. -1.  1. -1. -1. -1. -1.]\n",
            " [-1. -1.  1. -1.  1. -1. -1.  1.  1. -1.]\n",
            " [-1. -1. -1.  1.  1.  1.  1.  1. -1. -1.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as ste_sign_layer_call_fn, ste_sign_layer_call_and_return_conditional_losses, ste_sign_1_layer_call_fn, ste_sign_1_layer_call_and_return_conditional_losses, ste_sign_2_layer_call_fn while saving (showing 5 of 16). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Model saved with binarised weights.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining PUF key\n",
        "puf_key = np.random.randint(0, 2, size=256)\n",
        "\n",
        "# Column inversion function based on PUF key\n",
        "def apply_puf_column_inversion(model, puf_key):\n",
        "    for i, layer in enumerate(model.layers):\n",
        "        if isinstance(layer, lq.layers.QuantDense):\n",
        "            weights = layer.get_weights()\n",
        "            if not weights:\n",
        "                continue\n",
        "\n",
        "            W = weights[0]\n",
        "            out_features = W.shape[1]\n",
        "\n",
        "            # Creating a copy for manipulation\n",
        "            W_new = np.copy(W)\n",
        "\n",
        "            for k in range(0, out_features - 1, 2):\n",
        "                if puf_key[k % len(puf_key)] == 1:\n",
        "                    W_new[:, k], W_new[:, k+1] = W[:, k+1], W[:, k].copy()\n",
        "\n",
        "            layer.set_weights([W_new])\n",
        "\n",
        "            # Applying the same inversion to BatchNormalization layer\n",
        "            if i + 1 < len(model.layers) and isinstance(model.layers[i + 1], tf.keras.layers.BatchNormalization):\n",
        "                bn_weights = model.layers[i + 1].get_weights()\n",
        "                if len(bn_weights) == 4:\n",
        "                    gamma, beta, mean, var = bn_weights\n",
        "                    for k in range(0, out_features - 1, 2):\n",
        "                        if puf_key[k % len(puf_key)] == 1:\n",
        "                            gamma[k], gamma[k+1] = gamma[k+1], gamma[k]\n",
        "                            beta[k], beta[k+1] = beta[k+1], beta[k]\n",
        "                            mean[k], mean[k+1] = mean[k+1], mean[k]\n",
        "                            var[k], var[k+1] = var[k+1], var[k]\n",
        "                    model.layers[i + 1].set_weights([gamma, beta, mean, var])\n",
        "\n",
        "    print(\" Column inversion applied based on PUF key.\")"
      ],
      "metadata": {
        "id": "YtbmbkwYXzyZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying column inversion based on PUF key\n",
        "apply_puf_column_inversion(model, puf_key)\n",
        "\n",
        "# Printing weights after inversion\n",
        "print(\"\\n Binary Weights After Column Inversion:\")\n",
        "for layer in model.layers:\n",
        "    if isinstance(layer, lq.layers.QuantDense):\n",
        "        print(f\"\\nLayer: {layer.name}\")\n",
        "        binary_weights = get_binarized_weights(layer)\n",
        "        print(binary_weights[:10, :10])  # Print top-left 10x10 part of the binary weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tetc_CsJX3d_",
        "outputId": "accdda7b-517c-4e04-8af5-ce32ef6f0eea"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Column inversion applied based on PUF key.\n",
            "\n",
            " Binary Weights After Column Inversion:\n",
            "\n",
            "Layer: quant_dense\n",
            "[[-1. -1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
            " [-1.  1. -1.  1.  1. -1.  1. -1. -1.  1.]\n",
            " [ 1.  1.  1. -1. -1. -1. -1.  1. -1. -1.]\n",
            " [ 1.  1.  1.  1. -1. -1. -1.  1.  1.  1.]\n",
            " [ 1.  1. -1. -1.  1. -1. -1. -1. -1. -1.]\n",
            " [-1.  1. -1.  1.  1.  1.  1.  1.  1. -1.]\n",
            " [ 1.  1. -1.  1. -1.  1. -1. -1.  1. -1.]\n",
            " [-1. -1.  1.  1. -1.  1. -1. -1. -1.  1.]\n",
            " [-1. -1. -1.  1.  1. -1. -1.  1.  1. -1.]\n",
            " [-1. -1. -1. -1.  1. -1.  1. -1. -1.  1.]]\n",
            "\n",
            "Layer: quant_dense_1\n",
            "[[-1.  1.  1. -1.  1.  1. -1. -1.  1.  1.]\n",
            " [-1. -1. -1. -1.  1. -1.  1. -1. -1.  1.]\n",
            " [ 1. -1. -1.  1.  1. -1. -1.  1. -1.  1.]\n",
            " [-1. -1.  1. -1. -1. -1.  1.  1.  1. -1.]\n",
            " [ 1.  1. -1.  1. -1.  1.  1.  1.  1.  1.]\n",
            " [ 1.  1.  1. -1. -1. -1.  1.  1.  1. -1.]\n",
            " [ 1. -1. -1.  1.  1.  1. -1. -1. -1.  1.]\n",
            " [ 1.  1. -1.  1. -1. -1.  1. -1.  1. -1.]\n",
            " [-1. -1. -1. -1. -1. -1.  1. -1. -1. -1.]\n",
            " [-1.  1. -1.  1.  1.  1. -1. -1. -1.  1.]]\n",
            "\n",
            "Layer: quant_dense_2\n",
            "[[-1. -1. -1. -1.  1.  1.  1. -1. -1.  1.]\n",
            " [-1.  1.  1. -1.  1.  1. -1. -1.  1.  1.]\n",
            " [-1. -1.  1. -1. -1. -1.  1.  1.  1. -1.]\n",
            " [-1.  1.  1.  1. -1.  1.  1.  1. -1. -1.]\n",
            " [ 1. -1. -1. -1.  1.  1.  1. -1.  1.  1.]\n",
            " [ 1. -1.  1. -1. -1. -1. -1. -1.  1. -1.]\n",
            " [ 1.  1. -1.  1. -1. -1. -1.  1. -1. -1.]\n",
            " [-1.  1. -1.  1.  1. -1. -1. -1. -1.  1.]\n",
            " [-1.  1.  1.  1. -1. -1.  1. -1. -1.  1.]\n",
            " [-1. -1. -1.  1. -1. -1. -1.  1.  1. -1.]]\n",
            "\n",
            "Layer: quant_dense_3\n",
            "[[ 1.  1.  1.  1.  1.  1. -1.  1. -1. -1.]\n",
            " [ 1. -1. -1.  1.  1.  1.  1.  1. -1. -1.]\n",
            " [ 1.  1.  1.  1.  1. -1. -1. -1.  1. -1.]\n",
            " [-1. -1.  1.  1.  1. -1. -1.  1.  1.  1.]\n",
            " [ 1. -1.  1. -1. -1.  1.  1.  1. -1. -1.]\n",
            " [-1.  1.  1. -1. -1.  1.  1.  1. -1. -1.]\n",
            " [ 1.  1.  1. -1. -1. -1.  1.  1.  1. -1.]\n",
            " [ 1.  1. -1.  1.  1. -1. -1. -1. -1. -1.]\n",
            " [-1. -1. -1.  1. -1.  1. -1.  1.  1. -1.]\n",
            " [-1. -1.  1. -1.  1.  1.  1.  1. -1. -1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model after column inversion\n",
        "print(\"\\n Swapped Model Accuracy \")\n",
        "swapped_loss, swapped_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"Test Accuracy After Column Inversion: {swapped_acc*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrKbUxFdX8Gx",
        "outputId": "e63dfe9b-c4b9-49a6-cda1-c86547d28900"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Swapped Model Accuracy \n",
            "313/313 [==============================] - 3s 8ms/step - loss: 2.0958 - accuracy: 0.4203\n",
            "Test Accuracy After Column Inversion: 42.03%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the model with the column inversion applied\n",
        "os.makedirs(\"model_swapped\", exist_ok=True)\n",
        "model.save(\"model_swapped/binarized_model_swapped\", include_optimizer=False)\n",
        "print(\"\\n Model saved after column inversion.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "529Te4XCX_AO",
        "outputId": "1b8f8736-6b58-4c25-b2a0-a2666511f19c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as ste_sign_layer_call_fn, ste_sign_layer_call_and_return_conditional_losses, ste_sign_1_layer_call_fn, ste_sign_1_layer_call_and_return_conditional_losses, ste_sign_2_layer_call_fn while saving (showing 5 of 16). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Model saved after column inversion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to print the size of the weight matrix for each QuantDense layer\n",
        "def print_weight_shapes(model):\n",
        "    for i, layer in enumerate(model.layers):\n",
        "        if isinstance(layer, lq.layers.QuantDense):\n",
        "            weights = layer.get_weights()\n",
        "            if weights:\n",
        "                print(f\"Layer {i + 1} weight matrix shape: {weights[0].shape}\")\n",
        "\n",
        "# Printing the weight matrix shapes\n",
        "print_weight_shapes(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08emokUyYETl",
        "outputId": "7b01c715-512c-4148-e1c7-5c86bc52878a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1 weight matrix shape: (784, 512)\n",
            "Layer 3 weight matrix shape: (512, 512)\n",
            "Layer 5 weight matrix shape: (512, 512)\n",
            "Layer 7 weight matrix shape: (512, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For load the model later, the following code should be used:\n",
        "from larq.layers import QuantDense\n",
        "from larq.quantizers import ste_sign\n",
        "from larq.constraints import WeightClip\n",
        "\n",
        "custom_objects = {\n",
        "    \"QuantDense\": QuantDense,\n",
        "    \"ste_sign\": ste_sign,\n",
        "    \"WeightClip\": WeightClip\n",
        "}\n",
        "\n",
        "# Loading model with quantizers intact\n",
        "model = tf.keras.models.load_model(\"model_swapped/binarized_model_swapped\", custom_objects=custom_objects)"
      ],
      "metadata": {
        "id": "C4LaHmrGYCBT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}